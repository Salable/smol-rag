### SmolRAG Overview

SmolRAG is a lightweight retrieval‑augmented generation system inspired by LightRAG, focused on fast, up‑to‑date querying of your own documents.

Documents are split into ~2,000‑character overlapping chunks that keep Markdown code blocks intact. Long paragraphs are shortened at sentence boundaries so words never split. Each chunk is summarised to raise context quality before being embedded in NanoVectorDB, and its entities and relationships are stored in a local NetworkX knowledge graph for structured querying.

Change detection is automatic. Every file’s full content is hashed. If the path already exists but the hash changes, the old embeddings and graph entries are deleted and the new content is reingested, so answers always reflect the latest versions of the documents.

## Docs

For setup instructions and general guidance, please refer to [SmolRAG Docs](DOCS.md).

## Features

### Document Ingestion & Update Handling

Each document is split into overlapping chunks (approximately 2000 characters) using our `preserve_markdown_code_excerpts` function, which is optimised for code documentation by keeping code blocks intact.
- Markdown code blocks are preserved in their entirety, ensuring code examples remain functional and readable.
- Text is then segmented into paragraphs and, if necessary, further divided into sentences at whitespace boundaries to avoid splitting words.
- Each excerpt is then individually summarised with the whole document provided as context; the summary preserves details about where the excerpt fits in.
- Documents are embedded and checked for updates using hash-based deduplication. Each document is tracked using a combination of its file path and a hash of its content. If a file path already exists but the hash has changed, SmolRAG automatically removes the old version and re-ingests the updated content to ensure that queries reflect the most recent state of your source materials without unnecessary reprocessing.
- The ingestion process uses asyncio and gather for parallel processing, significantly improving data ingestion speed by processing multiple documents, generating embeddings, and extracting entities concurrently.

### Query Types

SmolRAG supports multiple query methods that leverage both semantic embeddings and a structured knowledge graph (KG). These methods are designed to balance accuracy, flexibility, and reasoning depth depending on the type of user query.

#### 1. **Vector Search Query** (`query`)

This method uses pure semantic similarity:

- The query is embedded into a vector using the OpenAI embedding API.
- The embedding is compared against excerpt embeddings in the vector DB.
- Top-k excerpts are selected and their summaries are retrieved.
- These excerpts and summaries form the context for a prompt to the LLM.

Use this when:
- You want quick results.
- The question is directly answerable from document content.
- You don’t need deeper relational understanding.

#### 2. **Local Knowledge Graph Query** (`local_kg_query`)

This method focuses on **low-level keywords** from the query:

- Embeds keywords to search for relevant entities in the KG.
- Ranks entities by graph degree and relevance.
- Extracts connected relationships and associated excerpts.
- Provides structured CSV context about entities, relationships, and text.

Use this when:
- You care about **fine-grained entity-level** information.
- You need a graph-aware answer that highlights links between terms.

#### 3. **Global Knowledge Graph Query** (`global_kg_query`)

This focuses on **high-level keywords**:

- Embeds and matches keywords with relationships in the KG.
- Retrieves top-ranked relationships based on weight and connectivity.
- Extracts linked entities and supporting excerpts.

Use this when:
- You're looking for a **bird’s-eye view** of topic interconnections.
- You want to reason about high-level concepts or broad themes.

#### 4. **Hybrid Knowledge Graph Query** (`hybrid_kg_query`)

A combination of **both low-level and high-level** keyword approaches:

- Combines local and global KG query outputs.
- Selects top entities, relationships, and excerpts.
- Presents them together to the LLM.

Use this when:
- You want the **best of both KG worlds**: precision + abstraction.
- You have a query that blends specific terms with general topics.

#### 5. **Mix Query** (`mix_query`)

This is the most comprehensive query type:

- Combines both **semantic search** (via vector embeddings) and **KG-based reasoning**.
- Retrieves relevant excerpts via vector search.
- Merges this with KG data from the hybrid KG query.
- Constructs a combined context for the LLM.

Use this when:
- You want **maximum context and coverage**.
- You have a complex query that benefits from both literal excerpts and conceptual links.
